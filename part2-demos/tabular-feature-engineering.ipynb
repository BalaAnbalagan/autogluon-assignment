{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Comparison - AutoGluon Presets\n",
    "\n",
    "## 🎯 Objective\n",
    "Compare AutoGluon performance across different quality presets to understand the **speed vs accuracy tradeoff**\n",
    "\n",
    "**Task**: Binary Classification  \n",
    "**Dataset**: Titanic  \n",
    "**Target**: `Survived`  \n",
    "**Comparison**: Different AutoGluon presets  \n",
    "\n",
    "## 📋 What This Notebook Does\n",
    "1. Load Titanic dataset\n",
    "2. Train with multiple presets (fast → slow, low → high quality)\n",
    "3. Compare performance, training time, and model complexity\n",
    "4. Show the impact of automatic feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "train = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/titanic/train.csv')\n",
    "test = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/titanic/test.csv')\n",
    "\n",
    "LABEL = \"Survived\"\n",
    "\n",
    "print(f\"✅ Data loaded!\")\n",
    "print(f\"   Train: {train.shape}\")\n",
    "print(f\"   Test:  {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏃 Preset 1: Optimize for Deployment (Fastest)\n",
    "\n",
    "Focus: **Speed and simplicity**\n",
    "- Fast training\n",
    "- Small model size\n",
    "- Fast inference\n",
    "- Limited feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏃 Training with 'optimize_for_deployment' preset...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "predictor_fast = TabularPredictor(\n",
    "    label=LABEL,\n",
    "    path=\"ag-fast\"\n",
    ").fit(\n",
    "    train,\n",
    "    presets=\"optimize_for_deployment\",\n",
    "    time_limit=180  # 3 minutes\n",
    ")\n",
    "\n",
    "time_fast = time.time() - start\n",
    "print(f\"\\n⏱️ Training time: {time_fast:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚖️ Preset 2: Medium Quality (Balanced)\n",
    "\n",
    "Focus: **Balance between speed and accuracy**\n",
    "- Moderate training time\n",
    "- Good performance\n",
    "- Reasonable model size\n",
    "- Some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⚖️ Training with 'medium_quality' preset...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "predictor_medium = TabularPredictor(\n",
    "    label=LABEL,\n",
    "    path=\"ag-medium\"\n",
    ").fit(\n",
    "    train,\n",
    "    presets=\"medium_quality\",\n",
    "    time_limit=300  # 5 minutes\n",
    ")\n",
    "\n",
    "time_medium = time.time() - start\n",
    "print(f\"\\n⏱️ Training time: {time_medium:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏆 Preset 3: Best Quality (Most Accurate)\n",
    "\n",
    "Focus: **Maximum accuracy**\n",
    "- Longer training time\n",
    "- Best performance\n",
    "- Complex ensembles\n",
    "- Extensive feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏆 Training with 'best_quality' preset...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "predictor_best = TabularPredictor(\n",
    "    label=LABEL,\n",
    "    path=\"ag-best\"\n",
    ").fit(\n",
    "    train,\n",
    "    presets=\"best_quality\",\n",
    "    time_limit=600  # 10 minutes\n",
    ")\n",
    "\n",
    "time_best = time.time() - start\n",
    "print(f\"\\n⏱️ Training time: {time_best:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Performance Comparison\n",
    "\n",
    "Compare all three presets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "perf_fast = predictor_fast.evaluate(train)\n",
    "perf_medium = predictor_medium.evaluate(train)\n",
    "perf_best = predictor_best.evaluate(train)\n",
    "\n",
    "# Get leaderboards\n",
    "lb_fast = predictor_fast.leaderboard(train, silent=True)\n",
    "lb_medium = predictor_medium.leaderboard(train, silent=True)\n",
    "lb_best = predictor_best.leaderboard(train, silent=True)\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Preset': ['optimize_for_deployment', 'medium_quality', 'best_quality'],\n",
    "    'Training Time (s)': [time_fast, time_medium, time_best],\n",
    "    'Accuracy': [\n",
    "        perf_fast.get('accuracy', 'N/A'),\n",
    "        perf_medium.get('accuracy', 'N/A'),\n",
    "        perf_best.get('accuracy', 'N/A')\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        perf_fast.get('roc_auc', 'N/A'),\n",
    "        perf_medium.get('roc_auc', 'N/A'),\n",
    "        perf_best.get('roc_auc', 'N/A')\n",
    "    ],\n",
    "    'Models Trained': [\n",
    "        len(lb_fast),\n",
    "        len(lb_medium),\n",
    "        len(lb_best)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"📊 Preset Comparison:\\n\")\n",
    "display(comparison)\n",
    "\n",
    "comparison.to_csv('preset_comparison.csv', index=False)\n",
    "print(\"\\n💾 Saved: preset_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Detailed Leaderboards\n",
    "\n",
    "View all models for each preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏃 OPTIMIZE_FOR_DEPLOYMENT Leaderboard:\")\n",
    "display(lb_fast.head(5))\n",
    "\n",
    "print(\"\\n⚖️ MEDIUM_QUALITY Leaderboard:\")\n",
    "display(lb_medium.head(5))\n",
    "\n",
    "print(\"\\n🏆 BEST_QUALITY Leaderboard:\")\n",
    "display(lb_best.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_fast = predictor_fast.feature_importance(train)\n",
    "fi_medium = predictor_medium.feature_importance(train)\n",
    "fi_best = predictor_best.feature_importance(train)\n",
    "\n",
    "print(\"🔍 Feature Importance - FAST:\")\n",
    "display(fi_fast)\n",
    "\n",
    "print(\"\\n🔍 Feature Importance - MEDIUM:\")\n",
    "display(fi_medium)\n",
    "\n",
    "print(\"\\n🔍 Feature Importance - BEST:\")\n",
    "display(fi_best)\n",
    "\n",
    "# Save all feature importances\n",
    "fi_fast.to_csv('feature_importance_fast.csv')\n",
    "fi_medium.to_csv('feature_importance_medium.csv')\n",
    "fi_best.to_csv('feature_importance_best.csv')\n",
    "print(\"\\n💾 Saved all feature importance files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Recommendations\n",
    "\n",
    "Based on the results, here's when to use each preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 PRESET RECOMMENDATIONS:\\n\")\n",
    "\n",
    "print(\"🏃 optimize_for_deployment:\")\n",
    "print(\"   ✓ Quick prototyping\")\n",
    "print(\"   ✓ Production deployments (fast inference)\")\n",
    "print(\"   ✓ Resource-constrained environments\")\n",
    "print(\"   ✓ When speed matters more than accuracy\")\n",
    "\n",
    "print(\"\\n⚖️ medium_quality:\")\n",
    "print(\"   ✓ Default choice for most use cases\")\n",
    "print(\"   ✓ Good balance of speed and accuracy\")\n",
    "print(\"   ✓ Exploratory data analysis\")\n",
    "print(\"   ✓ Time-limited experiments\")\n",
    "\n",
    "print(\"\\n🏆 best_quality:\")\n",
    "print(\"   ✓ Kaggle competitions\")\n",
    "print(\"   ✓ Critical applications (medical, finance)\")\n",
    "print(\"   ✓ When accuracy is paramount\")\n",
    "print(\"   ✓ Final production models\")\n",
    "print(\"   ✓ Benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Save All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('model_fast', 'zip', predictor_fast.path)\n",
    "shutil.make_archive('model_medium', 'zip', predictor_medium.path)\n",
    "shutil.make_archive('model_best', 'zip', predictor_best.path)\n",
    "\n",
    "print(\"✅ All models saved:\")\n",
    "print(\"   - model_fast.zip\")\n",
    "print(\"   - model_medium.zip\")\n",
    "print(\"   - model_best.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Three different AutoGluon quality presets\n",
    "2. ✅ Speed vs accuracy tradeoffs\n",
    "3. ✅ Impact of automatic feature engineering\n",
    "4. ✅ Model complexity comparison\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "| Preset | Speed | Accuracy | Use Case |\n",
    "|--------|-------|----------|----------|\n",
    "| optimize_for_deployment | ⚡⚡⚡ | ⭐⭐ | Quick experiments, production |\n",
    "| medium_quality | ⚡⚡ | ⭐⭐⭐ | Default choice |\n",
    "| best_quality | ⚡ | ⭐⭐⭐⭐ | Competitions, critical apps |\n",
    "\n",
    "**Typical Results on Titanic:**\n",
    "- Fast: ~78-80% accuracy in 1-2 minutes\n",
    "- Medium: ~81-83% accuracy in 3-5 minutes\n",
    "- Best: ~83-85% accuracy in 8-15 minutes\n",
    "\n",
    "**Next Steps:**\n",
    "- Try custom presets with `excluded_model_types` or `included_model_types`\n",
    "- Experiment with `num_bag_folds` for better ensembles\n",
    "- Use `hyperparameter_tune_kwargs` for advanced tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
