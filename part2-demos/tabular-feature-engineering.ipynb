{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Comparison - AutoGluon Presets\n",
    "\n",
    "## ğŸ¯ Objective\n",
    "Compare AutoGluon performance across different quality presets to understand the **speed vs accuracy tradeoff**\n",
    "\n",
    "**Task**: Binary Classification  \n",
    "**Dataset**: Titanic  \n",
    "**Target**: `Survived`  \n",
    "**Comparison**: Different AutoGluon presets  \n",
    "\n",
    "## ğŸ“‹ What This Notebook Does\n",
    "1. Load Titanic dataset\n",
    "2. Train with multiple presets (fast â†’ slow, low â†’ high quality)\n",
    "3. Compare performance, training time, and model complexity\n",
    "4. Show the impact of automatic feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "train = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/titanic/train.csv')\n",
    "test = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/titanic/test.csv')\n",
    "\n",
    "LABEL = \"Survived\"\n",
    "\n",
    "print(f\"âœ… Data loaded!\")\n",
    "print(f\"   Train: {train.shape}\")\n",
    "print(f\"   Test:  {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸƒ Preset 1: Optimize for Deployment (Fastest)\n",
    "\n",
    "Focus: **Speed and simplicity**\n",
    "- Fast training\n",
    "- Small model size\n",
    "- Fast inference\n",
    "- Limited feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸƒ Training with 'optimize_for_deployment' preset...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "predictor_fast = TabularPredictor(\n",
    "    label=LABEL,\n",
    "    path=\"ag-fast\"\n",
    ").fit(\n",
    "    train,\n",
    "    presets=\"optimize_for_deployment\",\n",
    "    time_limit=180  # 3 minutes\n",
    ")\n",
    "\n",
    "time_fast = time.time() - start\n",
    "print(f\"\\nâ±ï¸ Training time: {time_fast:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ Preset 2: Medium Quality (Balanced)\n",
    "\n",
    "Focus: **Balance between speed and accuracy**\n",
    "- Moderate training time\n",
    "- Good performance\n",
    "- Reasonable model size\n",
    "- Some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš–ï¸ Training with 'medium_quality' preset...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "predictor_medium = TabularPredictor(\n",
    "    label=LABEL,\n",
    "    path=\"ag-medium\"\n",
    ").fit(\n",
    "    train,\n",
    "    presets=\"medium_quality\",\n",
    "    time_limit=300  # 5 minutes\n",
    ")\n",
    "\n",
    "time_medium = time.time() - start\n",
    "print(f\"\\nâ±ï¸ Training time: {time_medium:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ† Preset 3: Best Quality (Most Accurate)\n",
    "\n",
    "Focus: **Maximum accuracy**\n",
    "- Longer training time\n",
    "- Best performance\n",
    "- Complex ensembles\n",
    "- Extensive feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ† Training with 'best_quality' preset...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "predictor_best = TabularPredictor(\n",
    "    label=LABEL,\n",
    "    path=\"ag-best\"\n",
    ").fit(\n",
    "    train,\n",
    "    presets=\"best_quality\",\n",
    "    time_limit=600  # 10 minutes\n",
    ")\n",
    "\n",
    "time_best = time.time() - start\n",
    "print(f\"\\nâ±ï¸ Training time: {time_best:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Performance Comparison\n",
    "\n",
    "Compare all three presets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "perf_fast = predictor_fast.evaluate(train)\n",
    "perf_medium = predictor_medium.evaluate(train)\n",
    "perf_best = predictor_best.evaluate(train)\n",
    "\n",
    "# Get leaderboards\n",
    "lb_fast = predictor_fast.leaderboard(train, silent=True)\n",
    "lb_medium = predictor_medium.leaderboard(train, silent=True)\n",
    "lb_best = predictor_best.leaderboard(train, silent=True)\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Preset': ['optimize_for_deployment', 'medium_quality', 'best_quality'],\n",
    "    'Training Time (s)': [time_fast, time_medium, time_best],\n",
    "    'Accuracy': [\n",
    "        perf_fast.get('accuracy', 'N/A'),\n",
    "        perf_medium.get('accuracy', 'N/A'),\n",
    "        perf_best.get('accuracy', 'N/A')\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        perf_fast.get('roc_auc', 'N/A'),\n",
    "        perf_medium.get('roc_auc', 'N/A'),\n",
    "        perf_best.get('roc_auc', 'N/A')\n",
    "    ],\n",
    "    'Models Trained': [\n",
    "        len(lb_fast),\n",
    "        len(lb_medium),\n",
    "        len(lb_best)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š Preset Comparison:\\n\")\n",
    "display(comparison)\n",
    "\n",
    "comparison.to_csv('preset_comparison.csv', index=False)\n",
    "print(\"\\nğŸ’¾ Saved: preset_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Detailed Leaderboards\n",
    "\n",
    "View all models for each preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸƒ OPTIMIZE_FOR_DEPLOYMENT Leaderboard:\")\n",
    "display(lb_fast.head(5))\n",
    "\n",
    "print(\"\\nâš–ï¸ MEDIUM_QUALITY Leaderboard:\")\n",
    "display(lb_medium.head(5))\n",
    "\n",
    "print(\"\\nğŸ† BEST_QUALITY Leaderboard:\")\n",
    "display(lb_best.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Feature Importance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_fast = predictor_fast.feature_importance(train)\n",
    "fi_medium = predictor_medium.feature_importance(train)\n",
    "fi_best = predictor_best.feature_importance(train)\n",
    "\n",
    "print(\"ğŸ” Feature Importance - FAST:\")\n",
    "display(fi_fast)\n",
    "\n",
    "print(\"\\nğŸ” Feature Importance - MEDIUM:\")\n",
    "display(fi_medium)\n",
    "\n",
    "print(\"\\nğŸ” Feature Importance - BEST:\")\n",
    "display(fi_best)\n",
    "\n",
    "# Save all feature importances\n",
    "fi_fast.to_csv('feature_importance_fast.csv')\n",
    "fi_medium.to_csv('feature_importance_medium.csv')\n",
    "fi_best.to_csv('feature_importance_best.csv')\n",
    "print(\"\\nğŸ’¾ Saved all feature importance files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Recommendations\n",
    "\n",
    "Based on the results, here's when to use each preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ PRESET RECOMMENDATIONS:\\n\")\n",
    "\n",
    "print(\"ğŸƒ optimize_for_deployment:\")\n",
    "print(\"   âœ“ Quick prototyping\")\n",
    "print(\"   âœ“ Production deployments (fast inference)\")\n",
    "print(\"   âœ“ Resource-constrained environments\")\n",
    "print(\"   âœ“ When speed matters more than accuracy\")\n",
    "\n",
    "print(\"\\nâš–ï¸ medium_quality:\")\n",
    "print(\"   âœ“ Default choice for most use cases\")\n",
    "print(\"   âœ“ Good balance of speed and accuracy\")\n",
    "print(\"   âœ“ Exploratory data analysis\")\n",
    "print(\"   âœ“ Time-limited experiments\")\n",
    "\n",
    "print(\"\\nğŸ† best_quality:\")\n",
    "print(\"   âœ“ Kaggle competitions\")\n",
    "print(\"   âœ“ Critical applications (medical, finance)\")\n",
    "print(\"   âœ“ When accuracy is paramount\")\n",
    "print(\"   âœ“ Final production models\")\n",
    "print(\"   âœ“ Benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Save All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('model_fast', 'zip', predictor_fast.path)\n",
    "shutil.make_archive('model_medium', 'zip', predictor_medium.path)\n",
    "shutil.make_archive('model_best', 'zip', predictor_best.path)\n",
    "\n",
    "print(\"âœ… All models saved:\")\n",
    "print(\"   - model_fast.zip\")\n",
    "print(\"   - model_medium.zip\")\n",
    "print(\"   - model_best.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. âœ… Three different AutoGluon quality presets\n",
    "2. âœ… Speed vs accuracy tradeoffs\n",
    "3. âœ… Impact of automatic feature engineering\n",
    "4. âœ… Model complexity comparison\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "| Preset | Speed | Accuracy | Use Case |\n",
    "|--------|-------|----------|----------|\n",
    "| optimize_for_deployment | âš¡âš¡âš¡ | â­â­ | Quick experiments, production |\n",
    "| medium_quality | âš¡âš¡ | â­â­â­ | Default choice |\n",
    "| best_quality | âš¡ | â­â­â­â­ | Competitions, critical apps |\n",
    "\n",
    "**Typical Results on Titanic:**\n",
    "- Fast: ~78-80% accuracy in 1-2 minutes\n",
    "- Medium: ~81-83% accuracy in 3-5 minutes\n",
    "- Best: ~83-85% accuracy in 8-15 minutes\n",
    "\n",
    "**Next Steps:**\n",
    "- Try custom presets with `excluded_model_types` or `included_model_types`\n",
    "- Experiment with `num_bag_folds` for better ensembles\n",
    "- Use `hyperparameter_tune_kwargs` for advanced tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
