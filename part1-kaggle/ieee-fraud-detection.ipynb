{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BalaAnbalagan/autogluon-assignment/blob/master/part1-kaggle/ieee-fraud-detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk14aTp858tE"
   },
   "source": [
    "# IEEE-CIS Fraud Detection (Binary Classification)\n",
    "\n",
    "## 🎯 Objective\n",
    "Build an AutoML binary classifier to detect fraudulent transactions using AutoGluon.\n",
    "\n",
    "**Task**: Binary Classification  \n",
    "**Dataset**: IEEE-CIS Fraud Detection (Kaggle)  \n",
    "**Target**: `isFraud`  \n",
    "**Metric**: ROC-AUC  \n",
    "\n",
    "## 📋 What This Notebook Does\n",
    "1. Install AutoGluon and dependencies\n",
    "2. Load transaction and identity data from Kaggle\n",
    "3. Merge datasets and prepare features\n",
    "4. Train AutoGluon predictor with automatic model selection\n",
    "5. Show leaderboard and feature importance\n",
    "6. Generate predictions and save artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvpDejAo58tF"
   },
   "source": [
    "## 📦 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUyXN0LC58tF"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q autogluon kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxLh_ACn58tG"
   },
   "source": [
    "## 📚 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2B-yymF258tG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4gNkvDp58tG"
   },
   "source": [
    "## 📥 Load Dataset\n",
    "\n",
    "### Option A: Kaggle API (Recommended)\n",
    "1. Go to https://www.kaggle.com/settings/account\n",
    "2. Click \"Create New API Token\" to download `kaggle.json`\n",
    "3. Upload it when prompted below\n",
    "\n",
    "### Option B: Manual Upload\n",
    "1. Download these 4 CSVs from [Kaggle Competition](https://www.kaggle.com/c/ieee-fraud-detection/data)\n",
    "   - train_transaction.csv\n",
    "   - train_identity.csv\n",
    "   - test_transaction.csv\n",
    "   - test_identity.csv\n",
    "2. Upload them when prompted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "18NGTQHH58tG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Checking data files...\n",
      "   ✓ data/train_transaction.csv (651.7 MB)\n",
      "   ✓ data/train_identity.csv (25.3 MB)\n",
      "   ✓ data/test_transaction.csv (584.8 MB)\n",
      "   ✓ data/test_identity.csv (24.6 MB)\n",
      "\n",
      "✅ All data files ready!\n"
     ]
    }
   ],
   "source": [
    "# Data files are already in the 'data' directory\n",
    "# No need to download - they've been copied from Downloads folder\n",
    "\n",
    "import os\n",
    "\n",
    "# Verify data files exist\n",
    "data_files = [\n",
    "    'data/train_transaction.csv',\n",
    "    'data/train_identity.csv', \n",
    "    'data/test_transaction.csv',\n",
    "    'data/test_identity.csv'\n",
    "]\n",
    "\n",
    "print(\"📂 Checking data files...\")\n",
    "for file in data_files:\n",
    "    if os.path.exists(file):\n",
    "        size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "        print(f\"   ✓ {file} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ✗ {file} NOT FOUND\")\n",
    "        \n",
    "print(\"\\n✅ All data files ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXJp2niy58tH"
   },
   "source": [
    "## 🎯 Set Target Label and Problem Type\n",
    "\n",
    "AutoGluon will automatically detect this is a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Load and Merge Data\n",
    "\n",
    "The dataset has two parts:\n",
    "- **Transaction data**: Payment details, amounts, cards\n",
    "- **Identity data**: Device and network information\n",
    "\n",
    "We'll merge them on `TransactionID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Loading transaction data...\n",
      "🔗 Merging datasets...\n",
      "\n",
      "✅ Data loaded successfully!\n",
      "   Train shape: (590540, 434)\n",
      "   Test shape: (506691, 433)\n",
      "\n",
      "📊 Target distribution:\n",
      "isFraud\n",
      "0    0.96501\n",
      "1    0.03499\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load transaction data\n",
    "print(\"📖 Loading transaction data...\")\n",
    "train_transaction = pd.read_csv('data/train_transaction.csv')\n",
    "train_identity = pd.read_csv('data/train_identity.csv')\n",
    "test_transaction = pd.read_csv('data/test_transaction.csv')\n",
    "test_identity = pd.read_csv('data/test_identity.csv')\n",
    "\n",
    "# Merge transaction and identity data\n",
    "print(\"🔗 Merging datasets...\")\n",
    "train = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "test = test_transaction.merge(test_identity, on='TransactionID', how='left')\n",
    "\n",
    "print(f\"\\n✅ Data loaded successfully!\")\n",
    "print(f\"   Train shape: {train.shape}\")\n",
    "print(f\"   Test shape: {test.shape}\")\n",
    "print(f\"\\n📊 Target distribution:\")\n",
    "print(train['isFraud'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WYxTWJB-58tH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Target Label: isFraud\n",
      "📈 Metric: ROC-AUC (auto-detected for binary classification)\n"
     ]
    }
   ],
   "source": [
    "# Define target label\n",
    "LABEL = \"isFraud\"\n",
    "\n",
    "# AutoGluon will auto-detect problem type (binary classification)\n",
    "# and use ROC-AUC as the metric\n",
    "print(f\"🎯 Target Label: {LABEL}\")\n",
    "print(f\"📈 Metric: ROC-AUC (auto-detected for binary classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7JzOSpf58tH"
   },
   "source": [
    "## 🚀 Train AutoGluon Model\n",
    "\n",
    "AutoGluon will:\n",
    "- Automatically handle missing values\n",
    "- Engineer features\n",
    "- Train multiple models (LightGBM, CatBoost, Neural Networks, etc.)\n",
    "- Create an ensemble of the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jnsQQAHp58tH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.9.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.0.0: Wed Sep 17 21:42:08 PDT 2025; root:xnu-12377.1.9~141/RELEASE_ARM64_T8132\n",
      "CPU Count:          10\n",
      "Memory Avail:       3.64 GB / 16.00 GB (22.8%)\n",
      "Disk Space Avail:   106.82 GB / 228.27 GB (46.8%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"/Users/banbalagan/Projects/autogluon-assignment/part1-kaggle/ag-1761508585-ieee-fraud\"\n",
      "Train Data Rows:    590540\n",
      "Train Data Columns: 433\n",
      "Label Column:       isFraud\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏋️ Training AutoGluon models...\n",
      "This may take 15-20 minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5469.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2590.15 MB (47.4% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 47.4% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 4): ['V28', 'V154', 'V155', 'V156']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 4 | ['V28', 'V154', 'V155', 'V156']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
      "\t\t('int', [])    :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
      "\t\t('object', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  31 | ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', ...]\n",
      "\t\t('float', [])    : 395 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n",
      "\t\t('int', [])      :   3 | ['TransactionID', 'TransactionDT', 'card1']\n",
      "\t13.0s = Fit runtime\n",
      "\t429 features in original data used to generate 429 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1811.77 MB (25.4% of available memory)\n",
      "\tWarning: Data size post feature transformation consumes 25.4% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "Data preprocessing and feature engineering runtime = 17.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 584634, Val Rows: 5906\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 882.87s of the 882.87s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 9.737 GB out of 3.080 GB available memory (316.096%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.56 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train LightGBMXT... Skipping this model.\n",
      "Fitting model: LightGBM ... Training model for up to 879.80s of the 879.80s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 9.737 GB out of 3.007 GB available memory (323.803%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.65 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train LightGBM... Skipping this model.\n",
      "Fitting model: RandomForestGini ... Training model for up to 877.72s of the 877.72s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.4/3.0 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 220 due to low memory. Expected memory usage reduced from 20.38% -> 15.0% of available memory...\n",
      "\t0.9366\t = Validation score   (roc_auc)\n",
      "\t112.36s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 765.18s of the 765.18s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.4/3.5 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 188 due to low memory. Expected memory usage reduced from 23.84% -> 15.0% of available memory...\n",
      "\t0.9355\t = Validation score   (roc_auc)\n",
      "\t77.5s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 687.39s of the 687.39s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 10.404 GB out of 3.157 GB available memory (329.564%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.35 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train CatBoost... Skipping this model.\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 685.03s of the 685.02s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.4/3.2 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 195 due to low memory. Expected memory usage reduced from 23.01% -> 15.0% of available memory...\n",
      "\t0.8989\t = Validation score   (roc_auc)\n",
      "\t67.07s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 617.79s of the 617.79s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.4/3.4 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 183 due to low memory. Expected memory usage reduced from 24.5% -> 15.0% of available memory...\n",
      "\t0.9152\t = Validation score   (roc_auc)\n",
      "\t63.72s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 553.86s of the 553.86s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 17.560 GB out of 3.272 GB available memory (536.695%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=6.01 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train NeuralNetFastAI... Skipping this model.\n",
      "Fitting model: XGBoost ... Training model for up to 551.53s of the 551.53s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 13.332 GB out of 3.309 GB available memory (402.918%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=4.08 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train XGBoost... Skipping this model.\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 549.47s of the 549.47s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 8.780 GB out of 3.305 GB available memory (265.665%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.00 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train NeuralNetTorch... Skipping this model.\n",
      "Fitting model: LightGBMLarge ... Training model for up to 548.26s of the 548.25s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 9.983 GB out of 3.303 GB available memory (302.199%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=3.41 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train LightGBMLarge... Skipping this model.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 547.13s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestGini': 0.667, 'RandomForestEntr': 0.333}\n",
      "\t0.9384\t = Validation score   (roc_auc)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 355.82s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 40475.1 rows/s (5906 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/banbalagan/Projects/autogluon-assignment/part1-kaggle/ag-1761508585-ieee-fraud\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Create save directory with timestamp\n",
    "save_dir = f\"ag-{int(time.time())}-ieee-fraud\"\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = TabularPredictor(\n",
    "    label=LABEL,\n",
    "    problem_type=\"binary\",  # Explicitly set for clarity\n",
    "    eval_metric=\"roc_auc\",  # ROC-AUC for binary classification\n",
    "    path=save_dir\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"🏋️ Training AutoGluon models...\")\n",
    "print(\"This may take 15-20 minutes...\\n\")\n",
    "\n",
    "predictor = predictor.fit(\n",
    "    train,\n",
    "    presets=\"medium_quality\",  # Balance between speed and accuracy\n",
    "    time_limit=900,            # 15 minutes (adjust as needed)\n",
    "    verbosity=2                # Show detailed progress\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88H8wxr658tH"
   },
   "source": [
    "## 📊 Model Leaderboard\n",
    "\n",
    "Shows all models trained and their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0JyrwICx58tH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Top 10 Models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.935450</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>6.349913</td>\n",
       "      <td>0.069796</td>\n",
       "      <td>77.495818</td>\n",
       "      <td>6.349913</td>\n",
       "      <td>0.069796</td>\n",
       "      <td>77.495818</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.938443</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>14.134962</td>\n",
       "      <td>0.145917</td>\n",
       "      <td>189.898799</td>\n",
       "      <td>0.008244</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.041871</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.936562</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>7.776805</td>\n",
       "      <td>0.075552</td>\n",
       "      <td>112.361110</td>\n",
       "      <td>7.776805</td>\n",
       "      <td>0.075552</td>\n",
       "      <td>112.361110</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesEntr</td>\n",
       "      <td>0.989869</td>\n",
       "      <td>0.915163</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>6.651335</td>\n",
       "      <td>0.086086</td>\n",
       "      <td>63.723692</td>\n",
       "      <td>6.651335</td>\n",
       "      <td>0.086086</td>\n",
       "      <td>63.723692</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ExtraTreesGini</td>\n",
       "      <td>0.986381</td>\n",
       "      <td>0.898914</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>6.975684</td>\n",
       "      <td>0.063401</td>\n",
       "      <td>67.072055</td>\n",
       "      <td>6.975684</td>\n",
       "      <td>0.063401</td>\n",
       "      <td>67.072055</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val eval_metric  pred_time_test  \\\n",
       "0     RandomForestEntr    0.999446   0.935450     roc_auc        6.349913   \n",
       "1  WeightedEnsemble_L2    0.999406   0.938443     roc_auc       14.134962   \n",
       "2     RandomForestGini    0.999200   0.936562     roc_auc        7.776805   \n",
       "3       ExtraTreesEntr    0.989869   0.915163     roc_auc        6.651335   \n",
       "4       ExtraTreesGini    0.986381   0.898914     roc_auc        6.975684   \n",
       "\n",
       "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       0.069796   77.495818                 6.349913                0.069796   \n",
       "1       0.145917  189.898799                 0.008244                0.000569   \n",
       "2       0.075552  112.361110                 7.776805                0.075552   \n",
       "3       0.086086   63.723692                 6.651335                0.086086   \n",
       "4       0.063401   67.072055                 6.975684                0.063401   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0          77.495818            1       True          2  \n",
       "1           0.041871            2       True          5  \n",
       "2         112.361110            1       True          1  \n",
       "3          63.723692            1       True          4  \n",
       "4          67.072055            1       True          3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saved: leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "# Get leaderboard\n",
    "leaderboard = predictor.leaderboard(train, silent=True)\n",
    "\n",
    "print(\"🏆 Top 10 Models:\")\n",
    "display(leaderboard.head(10))\n",
    "\n",
    "# Save leaderboard\n",
    "leaderboard.to_csv('leaderboard.csv', index=False)\n",
    "print(\"\\n💾 Saved: leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD-CmLcg58tH"
   },
   "source": [
    "## 🔍 Feature Importance\n",
    "\n",
    "Shows which features are most predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-pffgTXY58tH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['V28', 'V154', 'V155', 'V156']\n",
      "Computing feature importance via permutation shuffling for 429 features using 5000 rows with 5 shuffle sets...\n",
      "\t1529.47s\t= Expected runtime (305.89s per shuffle set)\n",
      "\t202.08s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Top 20 Most Important Features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TransactionAmt</th>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>0.000442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionDT</th>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>-0.000207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card1</th>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>-0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M5</th>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>-0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C13</th>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr1</th>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C11</th>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.010337</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>-0.000186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C6</th>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>-0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card2</th>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card6</th>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>-0.000178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D15</th>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>-0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C2</th>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.017770</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>-0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C8</th>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>-0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_02</th>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C14</th>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card5</th>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_emaildomain</th>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V307</th>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.030807</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>-0.000322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance    stddev   p_value  n  p99_high   p99_low\n",
       "TransactionAmt    0.003703  0.001584  0.003196  5  0.006964  0.000442\n",
       "C1                0.001206  0.000563  0.004359  5  0.002365  0.000047\n",
       "TransactionDT     0.001141  0.000655  0.008794  5  0.002489 -0.000207\n",
       "TransactionID     0.000987  0.000445  0.003867  5  0.001904  0.000070\n",
       "card1             0.000971  0.000688  0.017152  5  0.002387 -0.000445\n",
       "M5                0.000844  0.000470  0.007954  5  0.001812 -0.000123\n",
       "C13               0.000841  0.000283  0.001338  5  0.001425  0.000258\n",
       "addr1             0.000801  0.000271  0.001362  5  0.001360  0.000243\n",
       "C11               0.000771  0.000465  0.010337  5  0.001729 -0.000186\n",
       "C6                0.000771  0.000505  0.013425  5  0.001810 -0.000268\n",
       "card2             0.000734  0.000187  0.000469  5  0.001120  0.000348\n",
       "card6             0.000712  0.000432  0.010559  5  0.001601 -0.000178\n",
       "D15               0.000630  0.000358  0.008570  5  0.001368 -0.000108\n",
       "C2                0.000624  0.000447  0.017770  5  0.001544 -0.000297\n",
       "C8                0.000590  0.000310  0.006582  5  0.001229 -0.000049\n",
       "id_02             0.000533  0.000245  0.004136  5  0.001038  0.000028\n",
       "C14               0.000520  0.000184  0.001603  5  0.000898  0.000141\n",
       "card5             0.000513  0.000220  0.003229  5  0.000965  0.000060\n",
       "P_emaildomain     0.000490  0.000040  0.000005  5  0.000572  0.000408\n",
       "V307              0.000408  0.000355  0.030807  5  0.001139 -0.000322"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saved: feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance\n",
    "feature_importance = predictor.feature_importance(train)\n",
    "\n",
    "print(\"🔍 Top 20 Most Important Features:\")\n",
    "display(feature_importance.head(20))\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('feature_importance.csv')\n",
    "print(\"\\n💾 Saved: feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbQ78fp658tH"
   },
   "source": [
    "## 🔮 Generate Predictions\n",
    "\n",
    "Create submission file for Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A9eO8lUF58tH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed test column names to match training data\n",
      "🔮 Generating predictions...\n",
      "✅ Predictions generated!\n",
      "\n",
      "📊 Sample predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>0.015044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>0.022846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>0.050838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>0.007528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>0.009528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3663554</td>\n",
       "      <td>0.013515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3663555</td>\n",
       "      <td>0.082482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3663556</td>\n",
       "      <td>0.054694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3663557</td>\n",
       "      <td>0.010237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3663558</td>\n",
       "      <td>0.038265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID   isFraud\n",
       "0        3663549  0.015044\n",
       "1        3663550  0.022846\n",
       "2        3663551  0.050838\n",
       "3        3663552  0.007528\n",
       "4        3663553  0.009528\n",
       "5        3663554  0.013515\n",
       "6        3663555  0.082482\n",
       "7        3663556  0.054694\n",
       "8        3663557  0.010237\n",
       "9        3663558  0.038265"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saved: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Fix column name mismatch: change hyphens to underscores in test data\n",
    "test.columns = test.columns.str.replace('-', '_')\n",
    "print(\"✅ Fixed test column names to match training data\")\n",
    "# Predict probabilities for the positive class (fraud)\n",
    "print(\"🔮 Generating predictions...\")\n",
    "predictions = predictor.predict_proba(test)\n",
    "\n",
    "# For binary classification, get probability of class 1 (fraud)\n",
    "if isinstance(predictions, pd.DataFrame):\n",
    "    fraud_proba = predictions[1]  # Probability of fraud\n",
    "else:\n",
    "    fraud_proba = predictions\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'TransactionID': test['TransactionID'],\n",
    "    'isFraud': fraud_proba\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✅ Predictions generated!\")\n",
    "print(\"\\n📊 Sample predictions:\")\n",
    "display(submission.head(10))\n",
    "print(\"\\n💾 Saved: submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU8glqNC58tH"
   },
   "source": [
    "## 💾 Save Model Artifacts\n",
    "\n",
    "Package everything for download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uF20hdZy58tH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Creating model archive...\n",
      "\n",
      "✅ All artifacts saved!\n",
      "\n",
      "📥 Download these files:\n",
      "   ✓ autogluon_model.zip    - Trained model\n",
      "   ✓ leaderboard.csv         - Model comparison\n",
      "   ✓ feature_importance.csv  - Important features\n",
      "   ✓ submission.csv          - Kaggle submission\n",
      "\n",
      "💡 Use the Files panel (📁) to download\n"
     ]
    }
   ],
   "source": [
    "# Create model archive\n",
    "print(\"📦 Creating model archive...\")\n",
    "shutil.make_archive('autogluon_model', 'zip', save_dir)\n",
    "\n",
    "print(\"\\n✅ All artifacts saved!\")\n",
    "print(\"\\n📥 Download these files:\")\n",
    "print(\"   ✓ autogluon_model.zip    - Trained model\")\n",
    "print(\"   ✓ leaderboard.csv         - Model comparison\")\n",
    "print(\"   ✓ feature_importance.csv  - Important features\")\n",
    "print(\"   ✓ submission.csv          - Kaggle submission\")\n",
    "print(\"\\n💡 Use the Files panel (📁) to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_KNEYED58tI"
   },
   "source": [
    "## 🎓 Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Loading Kaggle competition data\n",
    "2. ✅ Merging transaction and identity datasets\n",
    "3. ✅ Training AutoGluon with automatic model selection\n",
    "4. ✅ Evaluating model performance via leaderboard\n",
    "5. ✅ Analyzing feature importance\n",
    "6. ✅ Generating Kaggle submission file\n",
    "\n",
    "**Next Steps:**\n",
    "- Submit `submission.csv` to Kaggle competition\n",
    "- Try different presets (`best_quality`, `high_quality`)\n",
    "- Increase `time_limit` for better results\n",
    "- Experiment with feature engineering"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "AutoGluon (venv)",
   "language": "python",
   "name": "autogluon-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
