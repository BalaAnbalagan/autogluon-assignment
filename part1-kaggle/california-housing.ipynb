{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Price Prediction (Regression)\n",
    "\n",
    "## üéØ Objective\n",
    "Build an AutoML regression model to predict median house values using AutoGluon.\n",
    "\n",
    "**Task**: Regression  \n",
    "**Dataset**: California Housing (sklearn built-in)  \n",
    "**Target**: `median_house_value`  \n",
    "**Metric**: RMSE (Root Mean Squared Error)  \n",
    "\n",
    "## üìã What This Notebook Does\n",
    "1. Install AutoGluon and dependencies\n",
    "2. Load California Housing dataset from sklearn\n",
    "3. Prepare features and target variable\n",
    "4. Train AutoGluon predictor for regression\n",
    "5. Show leaderboard and feature importance\n",
    "6. Generate predictions and save artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q autogluon scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load Dataset\n",
    "\n",
    "The California Housing dataset contains:\n",
    "- **20,640 samples** from California districts\n",
    "- **8 features**: Location, housing attributes, demographics\n",
    "- **Target**: Median house value (in $100,000s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading California Housing dataset...\n",
      "\n",
      "‚úÖ Data loaded successfully!\n",
      "   Shape: (20640, 9)\n",
      "\n",
      "üìä Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   MedInc              20640 non-null  float64\n",
      " 1   HouseAge            20640 non-null  float64\n",
      " 2   AveRooms            20640 non-null  float64\n",
      " 3   AveBedrms           20640 non-null  float64\n",
      " 4   Population          20640 non-null  float64\n",
      " 5   AveOccup            20640 non-null  float64\n",
      " 6   Latitude            20640 non-null  float64\n",
      " 7   Longitude           20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n",
      "None\n",
      "\n",
      "üìà Target Statistics:\n",
      "count    20640.000000\n",
      "mean         2.068558\n",
      "std          1.153956\n",
      "min          0.149990\n",
      "25%          1.196000\n",
      "50%          1.797000\n",
      "75%          2.647250\n",
      "max          5.000010\n",
      "Name: median_house_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load California Housing dataset\n",
    "print(\"üì• Loading California Housing dataset...\")\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create dataframe with features and target\n",
    "data = housing.frame\n",
    "\n",
    "# Rename target to be more descriptive\n",
    "data = data.rename(columns={'MedHouseVal': 'median_house_value'})\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"   Shape: {data.shape}\")\n",
    "print(f\"\\nüìä Dataset Info:\")\n",
    "print(data.info())\n",
    "print(f\"\\nüìà Target Statistics:\")\n",
    "print(data['median_house_value'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Train-Test Split\n",
    "\n",
    "Split data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data split:\n",
      "   Train: 16512 samples\n",
      "   Test:  4128 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"üìä Data split:\")\n",
    "print(f\"   Train: {train.shape[0]} samples\")\n",
    "print(f\"   Test:  {test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Set Target Label and Problem Type\n",
    "\n",
    "AutoGluon will automatically detect this is a regression problem because the target is numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Target Label: median_house_value\n",
      "üìà Metric: RMSE (auto-detected for regression)\n",
      "\n",
      "üìä Feature columns:\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"
     ]
    }
   ],
   "source": [
    "# Define target label\n",
    "LABEL = \"median_house_value\"\n",
    "\n",
    "# AutoGluon will auto-detect problem type (regression)\n",
    "# and use RMSE as the default metric\n",
    "print(f\"üéØ Target Label: {LABEL}\")\n",
    "print(f\"üìà Metric: RMSE (auto-detected for regression)\")\n",
    "print(f\"\\nüìä Feature columns:\")\n",
    "feature_cols = [col for col in train.columns if col != LABEL]\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Train AutoGluon Model\n",
    "\n",
    "AutoGluon will:\n",
    "- Automatically detect this is a regression task\n",
    "- Train multiple models (LightGBM, CatBoost, Neural Networks, etc.)\n",
    "- Create an ensemble of the best models\n",
    "- Optimize for RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.9.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.0.0: Wed Sep 17 21:42:08 PDT 2025; root:xnu-12377.1.9~141/RELEASE_ARM64_T8132\n",
      "CPU Count:          10\n",
      "Memory Avail:       5.59 GB / 16.00 GB (34.9%)\n",
      "Disk Space Avail:   109.79 GB / 228.27 GB (48.1%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"/Users/banbalagan/Projects/autogluon-assignment/part1-kaggle/ag-1761503699-california-housing\"\n",
      "Train Data Rows:    16512\n",
      "Train Data Columns: 8\n",
      "Label Column:       median_house_value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5643.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training AutoGluon models...\n",
      "This may take 10-15 minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t8 features in original data used to generate 8 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 14860, Val Rows: 1652\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 899.94s of the 899.94s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.0/5.2 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.490271\n",
      "[2000]\tvalid_set's rmse: 0.480824\n",
      "[3000]\tvalid_set's rmse: 0.479103\n",
      "[4000]\tvalid_set's rmse: 0.47806\n",
      "[5000]\tvalid_set's rmse: 0.477555\n",
      "[6000]\tvalid_set's rmse: 0.478424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4775\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.91s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 887.75s of the 887.75s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.0/5.6 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.45723\n",
      "[2000]\tvalid_set's rmse: 0.455822\n",
      "[3000]\tvalid_set's rmse: 0.454135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.454\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.86s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 881.74s of the 881.74s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0\n",
      "\t-0.5298\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.47s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 877.03s of the 877.03s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0\n",
      "\t-0.4356\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 846.98s of the 846.98s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0\n",
      "\t-0.5313\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 845.86s of the 845.85s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.0/5.3 GB\n",
      "\t-0.546\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 839.52s of the 839.52s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0\n",
      "\t-0.4589\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.43s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 834.03s of the 834.03s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.0/5.3 GB\n",
      "\t-0.517\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 801.08s of the 801.08s of remaining time.\n",
      "\tFitting with cpus=10, gpus=0, mem=0.0/5.4 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.453908\n",
      "[2000]\tvalid_set's rmse: 0.452343\n",
      "[3000]\tvalid_set's rmse: 0.452218\n",
      "[4000]\tvalid_set's rmse: 0.452185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4522\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.54s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 768.15s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.75, 'LightGBMLarge': 0.208, 'NeuralNetTorch': 0.042}\n",
      "\t-0.4336\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 131.88s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7596.6 rows/s (1652 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/banbalagan/Projects/autogluon-assignment/part1-kaggle/ag-1761503699-california-housing\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Create save directory with timestamp\n",
    "save_dir = f\"ag-{int(time.time())}-california-housing\"\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = TabularPredictor(\n",
    "    label=LABEL,\n",
    "    problem_type=\"regression\",  # Explicitly set for clarity\n",
    "    eval_metric=\"root_mean_squared_error\",  # RMSE for regression\n",
    "    path=save_dir\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"üèãÔ∏è Training AutoGluon models...\")\n",
    "print(\"This may take 10-15 minutes...\\n\")\n",
    "\n",
    "predictor = predictor.fit(\n",
    "    train,\n",
    "    presets=\"medium_quality\",  # Balance between speed and accuracy\n",
    "    time_limit=900,            # 15 minutes (adjust as needed)\n",
    "    verbosity=2                # Show detailed progress\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Model Leaderboard\n",
    "\n",
    "Shows all models trained and their performance (lower RMSE = better):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Top 10 Models (sorted by RMSE):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>-0.143494</td>\n",
       "      <td>-0.452150</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.114770</td>\n",
       "      <td>0.201329</td>\n",
       "      <td>32.544856</td>\n",
       "      <td>1.114770</td>\n",
       "      <td>0.201329</td>\n",
       "      <td>32.544856</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>-0.145659</td>\n",
       "      <td>-0.458902</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.492810</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>5.426123</td>\n",
       "      <td>0.492810</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>5.426123</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.179685</td>\n",
       "      <td>-0.433641</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.216885</td>\n",
       "      <td>0.217465</td>\n",
       "      <td>95.516262</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.010457</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>-0.185385</td>\n",
       "      <td>-0.454005</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.480925</td>\n",
       "      <td>0.076977</td>\n",
       "      <td>5.860347</td>\n",
       "      <td>0.480925</td>\n",
       "      <td>0.076977</td>\n",
       "      <td>5.860347</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>-0.194105</td>\n",
       "      <td>-0.435550</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.039579</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>30.027418</td>\n",
       "      <td>0.039579</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>30.027418</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestMSE</td>\n",
       "      <td>-0.243204</td>\n",
       "      <td>-0.529777</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.367702</td>\n",
       "      <td>0.063809</td>\n",
       "      <td>4.473133</td>\n",
       "      <td>0.367702</td>\n",
       "      <td>0.063809</td>\n",
       "      <td>4.473133</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTreesMSE</td>\n",
       "      <td>-0.244164</td>\n",
       "      <td>-0.531256</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.332478</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.911261</td>\n",
       "      <td>0.332478</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.911261</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>-0.277642</td>\n",
       "      <td>-0.477455</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.187048</td>\n",
       "      <td>0.177170</td>\n",
       "      <td>11.912182</td>\n",
       "      <td>1.187048</td>\n",
       "      <td>0.177170</td>\n",
       "      <td>11.912182</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeuralNetTorch</td>\n",
       "      <td>-0.431590</td>\n",
       "      <td>-0.517004</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.055437</td>\n",
       "      <td>0.010717</td>\n",
       "      <td>32.933531</td>\n",
       "      <td>0.055437</td>\n",
       "      <td>0.010717</td>\n",
       "      <td>32.933531</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>-0.527477</td>\n",
       "      <td>-0.546016</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.090616</td>\n",
       "      <td>0.013055</td>\n",
       "      <td>6.300323</td>\n",
       "      <td>0.090616</td>\n",
       "      <td>0.013055</td>\n",
       "      <td>6.300323</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val              eval_metric  \\\n",
       "0        LightGBMLarge   -0.143494  -0.452150  root_mean_squared_error   \n",
       "1              XGBoost   -0.145659  -0.458902  root_mean_squared_error   \n",
       "2  WeightedEnsemble_L2   -0.179685  -0.433641  root_mean_squared_error   \n",
       "3             LightGBM   -0.185385  -0.454005  root_mean_squared_error   \n",
       "4             CatBoost   -0.194105  -0.435550  root_mean_squared_error   \n",
       "5      RandomForestMSE   -0.243204  -0.529777  root_mean_squared_error   \n",
       "6        ExtraTreesMSE   -0.244164  -0.531256  root_mean_squared_error   \n",
       "7           LightGBMXT   -0.277642  -0.477455  root_mean_squared_error   \n",
       "8       NeuralNetTorch   -0.431590  -0.517004  root_mean_squared_error   \n",
       "9      NeuralNetFastAI   -0.527477  -0.546016  root_mean_squared_error   \n",
       "\n",
       "   pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  \\\n",
       "0        1.114770       0.201329  32.544856                 1.114770   \n",
       "1        0.492810       0.035739   5.426123                 0.492810   \n",
       "2        1.216885       0.217465  95.516262                 0.007099   \n",
       "3        0.480925       0.076977   5.860347                 0.480925   \n",
       "4        0.039579       0.004959  30.027418                 0.039579   \n",
       "5        0.367702       0.063809   4.473133                 0.367702   \n",
       "6        0.332478       0.048020   0.911261                 0.332478   \n",
       "7        1.187048       0.177170  11.912182                 1.187048   \n",
       "8        0.055437       0.010717  32.933531                 0.055437   \n",
       "9        0.090616       0.013055   6.300323                 0.090616   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.201329          32.544856            1       True   \n",
       "1                0.035739           5.426123            1       True   \n",
       "2                0.000460           0.010457            2       True   \n",
       "3                0.076977           5.860347            1       True   \n",
       "4                0.004959          30.027418            1       True   \n",
       "5                0.063809           4.473133            1       True   \n",
       "6                0.048020           0.911261            1       True   \n",
       "7                0.177170          11.912182            1       True   \n",
       "8                0.010717          32.933531            1       True   \n",
       "9                0.013055           6.300323            1       True   \n",
       "\n",
       "   fit_order  \n",
       "0          9  \n",
       "1          7  \n",
       "2         10  \n",
       "3          2  \n",
       "4          4  \n",
       "5          3  \n",
       "6          5  \n",
       "7          1  \n",
       "8          8  \n",
       "9          6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved: leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "# Get leaderboard\n",
    "leaderboard = predictor.leaderboard(train, silent=True)\n",
    "\n",
    "print(\"üèÜ Top 10 Models (sorted by RMSE):\")\n",
    "display(leaderboard.head(10))\n",
    "\n",
    "# Save leaderboard\n",
    "leaderboard.to_csv('leaderboard.csv', index=False)\n",
    "print(\"\\nüíæ Saved: leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Feature Importance\n",
    "\n",
    "Shows which features are most predictive of house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 8 features using 5000 rows with 5 shuffle sets...\n",
      "\t17.46s\t= Expected runtime (3.49s per shuffle set)\n",
      "\t16.62s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Feature Importance (all features):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>1.487997</td>\n",
       "      <td>0.021876</td>\n",
       "      <td>5.603919e-09</td>\n",
       "      <td>5</td>\n",
       "      <td>1.533039</td>\n",
       "      <td>1.442955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>1.392236</td>\n",
       "      <td>0.046787</td>\n",
       "      <td>1.528187e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>1.488571</td>\n",
       "      <td>1.295901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>0.489626</td>\n",
       "      <td>0.038122</td>\n",
       "      <td>4.374629e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.568120</td>\n",
       "      <td>0.411131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>0.301257</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>8.267509e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.357926</td>\n",
       "      <td>0.244587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>0.275909</td>\n",
       "      <td>0.026237</td>\n",
       "      <td>9.695745e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.329932</td>\n",
       "      <td>0.221886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>0.147702</td>\n",
       "      <td>0.022110</td>\n",
       "      <td>5.849344e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.193227</td>\n",
       "      <td>0.102178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>0.087790</td>\n",
       "      <td>0.017887</td>\n",
       "      <td>1.958250e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.124619</td>\n",
       "      <td>0.050961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>0.087620</td>\n",
       "      <td>0.015438</td>\n",
       "      <td>1.110185e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.119408</td>\n",
       "      <td>0.055833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            importance    stddev       p_value  n  p99_high   p99_low\n",
       "Latitude      1.487997  0.021876  5.603919e-09  5  1.533039  1.442955\n",
       "Longitude     1.392236  0.046787  1.528187e-07  5  1.488571  1.295901\n",
       "MedInc        0.489626  0.038122  4.374629e-06  5  0.568120  0.411131\n",
       "AveOccup      0.301257  0.027523  8.267509e-06  5  0.357926  0.244587\n",
       "AveRooms      0.275909  0.026237  9.695745e-06  5  0.329932  0.221886\n",
       "HouseAge      0.147702  0.022110  5.849344e-05  5  0.193227  0.102178\n",
       "Population    0.087790  0.017887  1.958250e-04  5  0.124619  0.050961\n",
       "AveBedrms     0.087620  0.015438  1.110185e-04  5  0.119408  0.055833"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved: feature_importance.csv\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance\n",
    "feature_importance = predictor.feature_importance(train)\n",
    "\n",
    "print(\"üîç Feature Importance (all features):\")\n",
    "display(feature_importance)\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('feature_importance.csv')\n",
    "print(\"\\nüíæ Saved: feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Model Performance on Test Set\n",
    "\n",
    "Evaluate the model on held-out test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating on test set...\n",
      "\n",
      "üìà Test Set Performance:\n",
      "   root_mean_squared_error: -0.4276\n",
      "   mean_squared_error: -0.1828\n",
      "   mean_absolute_error: -0.2746\n",
      "   r2: 0.8605\n",
      "   pearsonr: 0.9276\n",
      "   median_absolute_error: -0.1723\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üìä Evaluating on test set...\")\n",
    "test_performance = predictor.evaluate(test)\n",
    "\n",
    "print(\"\\nüìà Test Set Performance:\")\n",
    "for metric, value in test_performance.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Generate Predictions\n",
    "\n",
    "Make predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Generating predictions...\n",
      "‚úÖ Predictions generated!\n",
      "\n",
      "üìä Sample predictions (first 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>error</th>\n",
       "      <th>abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.47700</td>\n",
       "      <td>0.466602</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.010398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.45800</td>\n",
       "      <td>0.667731</td>\n",
       "      <td>-0.209731</td>\n",
       "      <td>0.209731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.00001</td>\n",
       "      <td>5.066875</td>\n",
       "      <td>-0.066865</td>\n",
       "      <td>0.066865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.18600</td>\n",
       "      <td>2.489718</td>\n",
       "      <td>-0.303718</td>\n",
       "      <td>0.303718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.78000</td>\n",
       "      <td>2.589109</td>\n",
       "      <td>0.190891</td>\n",
       "      <td>0.190891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.58700</td>\n",
       "      <td>1.639020</td>\n",
       "      <td>-0.052020</td>\n",
       "      <td>0.052020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.98200</td>\n",
       "      <td>2.335185</td>\n",
       "      <td>-0.353185</td>\n",
       "      <td>0.353185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.57500</td>\n",
       "      <td>1.557289</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.017711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.40000</td>\n",
       "      <td>3.079656</td>\n",
       "      <td>0.320344</td>\n",
       "      <td>0.320344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.46600</td>\n",
       "      <td>5.015294</td>\n",
       "      <td>-0.549294</td>\n",
       "      <td>0.549294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual  predicted     error  abs_error\n",
       "0  0.47700   0.466602  0.010398   0.010398\n",
       "1  0.45800   0.667731 -0.209731   0.209731\n",
       "2  5.00001   5.066875 -0.066865   0.066865\n",
       "3  2.18600   2.489718 -0.303718   0.303718\n",
       "4  2.78000   2.589109  0.190891   0.190891\n",
       "5  1.58700   1.639020 -0.052020   0.052020\n",
       "6  1.98200   2.335185 -0.353185   0.353185\n",
       "7  1.57500   1.557289  0.017711   0.017711\n",
       "8  3.40000   3.079656  0.320344   0.320344\n",
       "9  4.46600   5.015294 -0.549294   0.549294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved: predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "print(\"üîÆ Generating predictions...\")\n",
    "predictions = predictor.predict(test)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'actual': test[LABEL].values,\n",
    "    'predicted': predictions.values,\n",
    "    'error': test[LABEL].values - predictions.values,\n",
    "    'abs_error': abs(test[LABEL].values - predictions.values)\n",
    "})\n",
    "\n",
    "# Add feature columns for context\n",
    "for col in feature_cols:\n",
    "    comparison[col] = test[col].values\n",
    "\n",
    "comparison.to_csv('predictions.csv', index=False)\n",
    "print(\"‚úÖ Predictions generated!\")\n",
    "print(\"\\nüìä Sample predictions (first 10):\")\n",
    "display(comparison[['actual', 'predicted', 'error', 'abs_error']].head(10))\n",
    "print(\"\\nüíæ Saved: predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Model Artifacts\n",
    "\n",
    "Package everything for download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model archive\n",
    "print(\"üì¶ Creating model archive...\")\n",
    "shutil.make_archive('autogluon_model', 'zip', save_dir)\n",
    "\n",
    "print(\"\\n‚úÖ All artifacts saved!\")\n",
    "print(\"\\nüì• Download these files:\")\n",
    "print(\"   ‚úì autogluon_model.zip     - Trained model\")\n",
    "print(\"   ‚úì leaderboard.csv         - Model comparison\")\n",
    "print(\"   ‚úì feature_importance.csv  - Important features\")\n",
    "print(\"   ‚úì predictions.csv         - Test predictions with actuals\")\n",
    "print(\"\\nüí° Use the Files panel (üìÅ) to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ‚úÖ Loading California Housing dataset from sklearn\n",
    "2. ‚úÖ Training AutoGluon for regression task\n",
    "3. ‚úÖ Evaluating model performance (RMSE)\n",
    "4. ‚úÖ Analyzing feature importance\n",
    "5. ‚úÖ Generating predictions on test set\n",
    "\n",
    "**Key Insights:**\n",
    "- Most important features are typically: MedInc (median income), location (Latitude/Longitude)\n",
    "- AutoGluon automatically handles the regression task\n",
    "- Ensemble models typically perform best\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different presets (`best_quality`, `high_quality`)\n",
    "- Increase `time_limit` for better results\n",
    "- Experiment with feature engineering (e.g., adding distance from coast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
